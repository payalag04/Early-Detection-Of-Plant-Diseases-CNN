from keras.models import Sequential
from keras.layers.convolutional import Convolution2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.layers.core import Dense
import keras
from keras import regularizers, optimizers
from keras.layers import Conv2D,Input,Dense,MaxPooling2D,BatchNormalization,ZeroPadding2D,Flatten,Dropout
from keras.models import Model
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau, CSVLogger,EarlyStopping,ModelCheckpoint

import numpy as np
import matplotlib.pyplot as plt
import cv2
from numpy.random import permutation

np.random.seed(8)

def LeNet_Model():
   
    model = Sequential()
    
    
    model.add(Convolution2D(20, 5, 5, border_mode="same",input_shape=(60, 60,3))) 
    model.add(Dropout(0.2))
    model.add(Activation("relu"))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

  
    model.add(Convolution2D(50, 5, 5, border_mode="same"))
    model.add(Dropout(0.2))
    model.add(Activation("relu"))
    model.add(MaxPooling2D(pool_size=(2,2),strides=(2, 2),dim_ordering='th'))

    model.add(Flatten())
    model.add(Dense(500))
    model.add(Dropout(0.2))
    model.add(Activation("relu"))
    model.add(Dense(3))          
    model.add(Dropout(0.2))
    model.add(Activation("softmax"))
        
    return model
model= LeNet_Model()
model.summary()
train_images = np.load('train_images_lenet.npy')
train_labels = np.load('train_labels_lenet.npy')
train_images.shape 
train_labels.shape
np.unique(train_labels)
np.bincount(train_labels)

lr_reducer = ReduceLROnPlateau(factor = np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)

csv_logger = CSVLogger('Lenet.csv')

early_stopper = EarlyStopping(min_delta=0.001,patience=30)

model_checkpoint = ModelCheckpoint('Lenet.hdf5',monitor = 'val_loss', verbose = 1,save_best_only=True)


train_images = np.array(train_images)
train_labels = np.array(train_labels)
mean = np.mean(train_images,axis=(0,1,2,3))
std = np.std(train_images,axis=(0,1,2,3))
train_images = (train_images-mean)/(std+1e-7)
num_classes = 3
train_labels = np_utils.to_categorical(train_labels,num_classes)


train_images.shape, train_labels.shape



perm = permutation(len(train_images))
train_images = train_images[perm]
train_labels = train_labels[perm]
new_train = train_images[1:40]
new_labels = train_labels[1:40]
val_images= train_images[40:]
val_labels = train_labels[40:]


new_train.shape,val_images.shape



model.compile(loss='categorical_crossentropy',
        optimizer="Adam",
        metrics=['accuracy'])





datagen = ImageDataGenerator(
        featurewise_center=False, 
        samplewise_center=False,  
        featurewise_std_normalization=False,  
        samplewise_std_normalization=False,  
        zca_whitening=False,  
        rotation_range=20,  
        width_shift_range=0.1,  
        height_shift_range=0.1, 
        horizontal_flip=True,  
        vertical_flip=False)  


datagen.fit(new_train)
model.fit_generator(datagen.flow(new_train, new_labels, batch_size=12),
                        steps_per_epoch=new_train.shape[0] // 12,
                        epochs=30,
                        verbose=1,
                        validation_data=(val_images,val_labels))



model.save('Tomato_diseases.h5')



from keras.models import load_model

loaded_model= load_model('Tomato_diseases.h5')
loaded_model


class_names = {1:'Healthy',0:'early_blight',2:'late_blight'}



input_img= cv2.imread("C://Users//HP//Documents//loginware//major_project//database//Healthy//h1.png", 1)
if input_img is not None:
    img = cv2.resize(input_img, (60,60)).astype(np.float32)
            
else:
    print("image not loaded")
    
img = (img-mean)/(std+1e-7)
img = np.expand_dims(img, axis=0)  
out = loaded_model.predict(img) 
print(out)
print(np.argmax(out))
print(class_names[np.argmax(out)])



input_img= cv2.imread(r"C:/Users/HP/Documents/loginware/major_project/image_dataset/early_blight/download.jpg", 1)

if input_img is not None:
    img = cv2.resize(input_img,(60,60)).astype(np.float32)
else:
    print("image not loaded")
    
img = (img-mean)/(std+1e-7)
img = np.expand_dims(img, axis=0)  
out = loaded_model.predict(img) 
print(out)
print(np.argmax(out))
print(class_names[np.argmax(out)])


input_img= cv2.imread(r"C:/Users/HP/Documents/loginware/major_project/image_dataset/late_blight/download.jpg", 1)

if input_img is not None:
    img = cv2.resize(input_img,(60,60)).astype(np.float32)
else:
    print("image not loaded")
    
img = (img-mean)/(std+1e-7)
img = np.expand_dims(img, axis=0)  
out = loaded_model.predict(img) 
print(out)
print(np.argmax(out))
print(class_names[np.argmax(out)])
